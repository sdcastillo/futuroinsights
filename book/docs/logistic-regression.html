<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 13 Logistic Regression | Exam PA R Study Manual</title>
  <meta name="description" content=" 13 Logistic Regression | Exam PA R Study Manual" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content=" 13 Logistic Regression | Exam PA R Study Manual" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="sdcastillo/PA-R-Study-Manual" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 13 Logistic Regression | Exam PA R Study Manual" />
  
  
  

<meta name="author" content="Sam Castillo" />


<meta name="date" content="2020-01-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="images/artificial_actuary_logo_favicon.png" type="image/x-icon" />
<link rel="prev" href="glms-for-classification.html"/>
<link rel="next" href="penalized-linear-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Exam PA Study Manual</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> How to use this book</a></li>
<li class="chapter" data-level="2" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>2</b> The exam</a></li>
<li class="chapter" data-level="3" data-path="prometric-demo.html"><a href="prometric-demo.html"><i class="fa fa-check"></i><b>3</b> Prometric Demo</a></li>
<li class="chapter" data-level="4" data-path="you-already-know-what-learning-is.html"><a href="you-already-know-what-learning-is.html"><i class="fa fa-check"></i><b>4</b> You already know what learning is</a></li>
<li class="chapter" data-level="5" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>5</b> Getting started</a><ul>
<li class="chapter" data-level="5.1" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>5.1</b> Download the data</a></li>
<li class="chapter" data-level="5.2" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>5.2</b> Download ISLR</a></li>
<li class="chapter" data-level="5.3" data-path="getting-started.html"><a href="getting-started.html#new-users"><i class="fa fa-check"></i><b>5.3</b> New users</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>6</b> R programming</a><ul>
<li class="chapter" data-level="6.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>6.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="6.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>6.2</b> Basic operations</a></li>
<li class="chapter" data-level="6.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>6.3</b> Lists</a></li>
<li class="chapter" data-level="6.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>6.4</b> Functions</a></li>
<li class="chapter" data-level="6.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>6.5</b> Data frames</a></li>
<li class="chapter" data-level="6.6" data-path="r-programming.html"><a href="r-programming.html#pipes"><i class="fa fa-check"></i><b>6.6</b> Pipes</a></li>
<li class="chapter" data-level="6.7" data-path="r-programming.html"><a href="r-programming.html#the-soas-code-doesnt-use-pipes-or-dplyr-so-can-i-skip-learning-this"><i class="fa fa-check"></i><b>6.7</b> The SOA’s code doesn’t use pipes or dplyr, so can I skip learning this?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>7</b> Data manipulation</a><ul>
<li class="chapter" data-level="7.1" data-path="data-manipulation.html"><a href="data-manipulation.html#look-at-the-data"><i class="fa fa-check"></i><b>7.1</b> Look at the data</a></li>
<li class="chapter" data-level="7.2" data-path="data-manipulation.html"><a href="data-manipulation.html#transform-the-data"><i class="fa fa-check"></i><b>7.2</b> Transform the data</a></li>
<li class="chapter" data-level="7.3" data-path="data-manipulation.html"><a href="data-manipulation.html#exercises"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
<li class="chapter" data-level="7.4" data-path="data-manipulation.html"><a href="data-manipulation.html#answers-to-exercises"><i class="fa fa-check"></i><b>7.4</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>8</b> Visualization</a><ul>
<li class="chapter" data-level="8.1" data-path="visualization.html"><a href="visualization.html#create-a-plot-object-ggplot"><i class="fa fa-check"></i><b>8.1</b> Create a plot object (ggplot)</a></li>
<li class="chapter" data-level="8.2" data-path="visualization.html"><a href="visualization.html#add-a-plot"><i class="fa fa-check"></i><b>8.2</b> Add a plot</a></li>
<li class="chapter" data-level="8.3" data-path="visualization.html"><a href="visualization.html#data-manipulation-chaining"><i class="fa fa-check"></i><b>8.3</b> Data manipulation chaining</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html"><i class="fa fa-check"></i><b>9</b> Introduction to Modeling</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-vocabulary"><i class="fa fa-check"></i><b>9.1</b> Modeling Vocabulary</a></li>
<li class="chapter" data-level="9.2" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#modeling-notation"><i class="fa fa-check"></i><b>9.2</b> Modeling Notation</a></li>
<li class="chapter" data-level="9.3" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>9.3</b> Ordinary least squares (OLS)</a></li>
<li class="chapter" data-level="9.4" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#regression-vs.-classification"><i class="fa fa-check"></i><b>9.4</b> Regression vs. Classification</a></li>
<li class="chapter" data-level="9.5" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#performance-metrics"><i class="fa fa-check"></i><b>9.5</b> Performance Metrics</a></li>
<li class="chapter" data-level="9.6" data-path="introduction-to-modeling.html"><a href="introduction-to-modeling.html#example"><i class="fa fa-check"></i><b>9.6</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generalized-linear-models-glms.html"><a href="generalized-linear-models-glms.html"><i class="fa fa-check"></i><b>10</b> Generalized linear models (GLMs)</a></li>
<li class="chapter" data-level="11" data-path="glms-for-regression.html"><a href="glms-for-regression.html"><i class="fa fa-check"></i><b>11</b> GLMs for regression</a></li>
<li class="chapter" data-level="12" data-path="glms-for-classification.html"><a href="glms-for-classification.html"><i class="fa fa-check"></i><b>12</b> GLMs for classification</a><ul>
<li class="chapter" data-level="12.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#binary-target"><i class="fa fa-check"></i><b>12.1</b> Binary target</a></li>
<li class="chapter" data-level="12.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#count-target"><i class="fa fa-check"></i><b>12.2</b> Count target</a></li>
<li class="chapter" data-level="12.3" data-path="glms-for-classification.html"><a href="glms-for-classification.html#link-functions"><i class="fa fa-check"></i><b>12.3</b> Link Functions</a></li>
<li class="chapter" data-level="12.4" data-path="glms-for-classification.html"><a href="glms-for-classification.html#interpretation-of-coefficients"><i class="fa fa-check"></i><b>12.4</b> Interpretation of coefficients</a><ul>
<li class="chapter" data-level="12.4.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#log-link"><i class="fa fa-check"></i><b>12.4.1</b> Log link</a></li>
<li class="chapter" data-level="12.4.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#logit"><i class="fa fa-check"></i><b>12.4.2</b> Logit</a></li>
<li class="chapter" data-level="12.4.3" data-path="glms-for-classification.html"><a href="glms-for-classification.html#other-link-functions"><i class="fa fa-check"></i><b>12.4.3</b> Other link functions</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="glms-for-classification.html"><a href="glms-for-classification.html#link-function-summary"><i class="fa fa-check"></i><b>12.5</b> Link function summary</a></li>
<li class="chapter" data-level="12.6" data-path="glms-for-classification.html"><a href="glms-for-classification.html#residuals"><i class="fa fa-check"></i><b>12.6</b> Residuals</a></li>
<li class="chapter" data-level="12.7" data-path="glms-for-classification.html"><a href="glms-for-classification.html#example-1"><i class="fa fa-check"></i><b>12.7</b> Example</a></li>
<li class="chapter" data-level="12.8" data-path="glms-for-classification.html"><a href="glms-for-classification.html#combinations-of-link-functions-and-target-distributions"><i class="fa fa-check"></i><b>12.8</b> Combinations of Link Functions and Target Distributions</a><ul>
<li class="chapter" data-level="12.8.1" data-path="glms-for-classification.html"><a href="glms-for-classification.html#gaussian-response-with-log-link"><i class="fa fa-check"></i><b>12.8.1</b> Gaussian Response with Log Link</a></li>
<li class="chapter" data-level="12.8.2" data-path="glms-for-classification.html"><a href="glms-for-classification.html#gaussian-response-with-inverse-link"><i class="fa fa-check"></i><b>12.8.2</b> Gaussian Response with Inverse Link</a></li>
<li class="chapter" data-level="12.8.3" data-path="glms-for-classification.html"><a href="glms-for-classification.html#gaussian-response-with-identity-link"><i class="fa fa-check"></i><b>12.8.3</b> Gaussian Response with Identity Link</a></li>
<li class="chapter" data-level="12.8.4" data-path="glms-for-classification.html"><a href="glms-for-classification.html#gaussian-response-with-log-link-and-negative-values"><i class="fa fa-check"></i><b>12.8.4</b> Gaussian Response with Log Link and Negative Values</a></li>
<li class="chapter" data-level="12.8.5" data-path="glms-for-classification.html"><a href="glms-for-classification.html#gamma-response-with-log-link"><i class="fa fa-check"></i><b>12.8.5</b> Gamma Response with Log Link</a></li>
<li class="chapter" data-level="12.8.6" data-path="glms-for-classification.html"><a href="glms-for-classification.html#gamma-with-inverse-link"><i class="fa fa-check"></i><b>12.8.6</b> Gamma with Inverse Link</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="glms-for-classification.html"><a href="glms-for-classification.html#log-transforms-of-continuous-predictors"><i class="fa fa-check"></i><b>12.9</b> Log transforms of continuous predictors</a></li>
<li class="chapter" data-level="12.10" data-path="glms-for-classification.html"><a href="glms-for-classification.html#reference-levels"><i class="fa fa-check"></i><b>12.10</b> Reference levels</a></li>
<li class="chapter" data-level="12.11" data-path="glms-for-classification.html"><a href="glms-for-classification.html#interactions"><i class="fa fa-check"></i><b>12.11</b> Interactions</a></li>
<li class="chapter" data-level="12.12" data-path="glms-for-classification.html"><a href="glms-for-classification.html#offsets"><i class="fa fa-check"></i><b>12.12</b> Offsets</a></li>
<li class="chapter" data-level="12.13" data-path="glms-for-classification.html"><a href="glms-for-classification.html#tweedie-regression"><i class="fa fa-check"></i><b>12.13</b> Tweedie regression</a></li>
<li class="chapter" data-level="12.14" data-path="glms-for-classification.html"><a href="glms-for-classification.html#stepwise-subset-selection"><i class="fa fa-check"></i><b>12.14</b> Stepwise subset selection</a></li>
<li class="chapter" data-level="12.15" data-path="glms-for-classification.html"><a href="glms-for-classification.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>12.15</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>13</b> Logistic Regression</a><ul>
<li class="chapter" data-level="13.1" data-path="logistic-regression.html"><a href="logistic-regression.html#model-form"><i class="fa fa-check"></i><b>13.1</b> Model form</a></li>
<li class="chapter" data-level="13.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-2"><i class="fa fa-check"></i><b>13.2</b> Example</a></li>
<li class="chapter" data-level="13.3" data-path="logistic-regression.html"><a href="logistic-regression.html#classification-metrics"><i class="fa fa-check"></i><b>13.3</b> Classification metrics</a><ul>
<li class="chapter" data-level="13.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#area-under-the-roc-curv-auc"><i class="fa fa-check"></i><b>13.3.1</b> Area Under the ROC Curv (AUC)</a></li>
<li class="chapter" data-level="13.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#additional-reading"><i class="fa fa-check"></i><b>13.3.2</b> Additional reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html"><i class="fa fa-check"></i><b>14</b> Penalized Linear Models</a><ul>
<li class="chapter" data-level="14.1" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>14.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="14.2" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#lasso"><i class="fa fa-check"></i><b>14.2</b> Lasso</a></li>
<li class="chapter" data-level="14.3" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#elastic-net"><i class="fa fa-check"></i><b>14.3</b> Elastic Net</a></li>
<li class="chapter" data-level="14.4" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>14.4</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="14.5" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#example-ridge-regression"><i class="fa fa-check"></i><b>14.5</b> Example: Ridge Regression</a></li>
<li class="chapter" data-level="14.6" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#example-the-lasso"><i class="fa fa-check"></i><b>14.6</b> Example: The Lasso</a></li>
<li class="chapter" data-level="14.7" data-path="penalized-linear-models.html"><a href="penalized-linear-models.html#references"><i class="fa fa-check"></i><b>14.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>15</b> Tree-based models</a><ul>
<li class="chapter" data-level="15.1" data-path="tree-based-models.html"><a href="tree-based-models.html#decision-trees"><i class="fa fa-check"></i><b>15.1</b> Decision Trees</a><ul>
<li class="chapter" data-level="15.1.1" data-path="tree-based-models.html"><a href="tree-based-models.html#model-form-1"><i class="fa fa-check"></i><b>15.1.1</b> Model form</a></li>
<li class="chapter" data-level="15.1.2" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-2"><i class="fa fa-check"></i><b>15.1.2</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="tree-based-models.html"><a href="tree-based-models.html#ensemble-learning"><i class="fa fa-check"></i><b>15.2</b> Ensemble learning</a><ul>
<li class="chapter" data-level="15.2.1" data-path="tree-based-models.html"><a href="tree-based-models.html#bagging"><i class="fa fa-check"></i><b>15.2.1</b> Bagging</a></li>
<li class="chapter" data-level="15.2.2" data-path="tree-based-models.html"><a href="tree-based-models.html#boosting"><i class="fa fa-check"></i><b>15.2.2</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>15.3</b> Random Forests</a><ul>
<li class="chapter" data-level="15.3.1" data-path="tree-based-models.html"><a href="tree-based-models.html#model-form-2"><i class="fa fa-check"></i><b>15.3.1</b> Model form</a></li>
<li class="chapter" data-level="15.3.2" data-path="tree-based-models.html"><a href="tree-based-models.html#example-3"><i class="fa fa-check"></i><b>15.3.2</b> Example</a></li>
<li class="chapter" data-level="15.3.3" data-path="tree-based-models.html"><a href="tree-based-models.html#variable-importance"><i class="fa fa-check"></i><b>15.3.3</b> Variable Importance</a></li>
<li class="chapter" data-level="15.3.4" data-path="tree-based-models.html"><a href="tree-based-models.html#partial-dependence"><i class="fa fa-check"></i><b>15.3.4</b> Partial dependence</a></li>
<li class="chapter" data-level="15.3.5" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-3"><i class="fa fa-check"></i><b>15.3.5</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>15.4</b> Gradient Boosted Trees</a><ul>
<li class="chapter" data-level="15.4.1" data-path="tree-based-models.html"><a href="tree-based-models.html#parameters"><i class="fa fa-check"></i><b>15.4.1</b> Parameters</a></li>
<li class="chapter" data-level="15.4.2" data-path="tree-based-models.html"><a href="tree-based-models.html#example-4"><i class="fa fa-check"></i><b>15.4.2</b> Example</a></li>
<li class="chapter" data-level="15.4.3" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages-4"><i class="fa fa-check"></i><b>15.4.3</b> Advantages and disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="tree-based-models.html"><a href="tree-based-models.html#exercises-1"><i class="fa fa-check"></i><b>15.5</b> Exercises</a><ul>
<li class="chapter" data-level="15.5.1" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-with-randomforest"><i class="fa fa-check"></i><b>15.5.1</b> 1. RF with <code>randomForest</code></a></li>
<li class="chapter" data-level="15.5.2" data-path="tree-based-models.html"><a href="tree-based-models.html#rf-tuning-with-caret"><i class="fa fa-check"></i><b>15.5.2</b> 2. RF tuning with <code>caret</code></a></li>
<li class="chapter" data-level="15.5.3" data-path="tree-based-models.html"><a href="tree-based-models.html#tuning-a-gbm-with-caret"><i class="fa fa-check"></i><b>15.5.3</b> 3. Tuning a GBM with <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="tree-based-models.html"><a href="tree-based-models.html#answers-to-exercises-1"><i class="fa fa-check"></i><b>15.6</b> Answers to Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>16</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="16.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-componant-analysis-pca"><i class="fa fa-check"></i><b>16.1</b> Principal Componant Analysis (PCA)</a><ul>
<li class="chapter" data-level="16.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-us-arrests"><i class="fa fa-check"></i><b>16.1.1</b> Example: PCA on US Arrests</a></li>
<li class="chapter" data-level="16.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-pca-on-cancel-cells"><i class="fa fa-check"></i><b>16.1.2</b> Example: PCA on Cancel Cells</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>16.2</b> Clustering</a><ul>
<li class="chapter" data-level="16.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>16.2.1</b> K-Means Clustering</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering"><i class="fa fa-check"></i><b>16.3</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="16.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#example-clustering-cancel-cells"><i class="fa fa-check"></i><b>16.3.1</b> Example: Clustering Cancel Cells</a></li>
<li class="chapter" data-level="16.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references-1"><i class="fa fa-check"></i><b>16.3.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="practice-exams.html"><a href="practice-exams.html"><i class="fa fa-check"></i><b>17</b> Practice Exams</a></li>
<li class="chapter" data-level="18" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i><b>18</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Exam PA R Study Manual</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1">
<h1><span class="header-section-number"> 13</span> Logistic Regression</h1>
<div id="model-form" class="section level2">
<h2><span class="header-section-number">13.1</span> Model form</h2>
<p>Logistic regression is a special type of GLM. The name is confusing because the objective is <em>classification</em> and not regression. While most examples focus on binary classification, logistic regression also works for multiclass classification.</p>
<p>The model form is as before</p>
<p><span class="math display">\[g(\mathbf{\hat{y}}) = \mathbf{X} \mathbf{\beta}\]</span></p>
<p>However, now the target <span class="math inline">\(y_i\)</span> is a category. Our objective is to predict a probability of being in each category. For regression, <span class="math inline">\(\hat{y_i}\)</span> can be any number, but now we need <span class="math inline">\(0 \leq \hat{y_i} \leq 1\)</span>.</p>
<p>We can use a special link function, known as the <em>standard logistic function</em>, <em>sigmoid</em>, or <em>logit</em>, to force the output to be in this range of <span class="math inline">\(\{0,1\}\)</span>.</p>
<p><span class="math display">\[\mathbf{\hat{y}} = g^{-1}(\mathbf{X} \mathbf{\beta}) = \frac{1}{1 + e^{-\mathbf{X} \mathbf{\beta}}}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-52"></span>
<img src="05-linear-models_files/figure-html/unnamed-chunk-52-1.png" alt="Standard Logistic Function" width="384" />
<p class="caption">
Figure 13.1: Standard Logistic Function
</p>
</div>
<p>Other link functions for classification problems are possible as well, although the logistic function is the most common. If a problem asks for an alternative link, such as the <em>probit</em>, fit both models and compare the performance.</p>
</div>
<div id="example-2" class="section level2">
<h2><span class="header-section-number">13.2</span> Example</h2>
<p>Using the <code>auto_claim</code> data, we predict whether or not a policy has a claim. This is also known as the <em>claim frequency</em>.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" title="1">auto_claim <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(CLM_FLAG)</a></code></pre></div>
<pre><code>## # A tibble: 2 x 2
##   CLM_FLAG     n
##   &lt;chr&gt;    &lt;int&gt;
## 1 No        7556
## 2 Yes       2740</code></pre>
<p>About 40% do not have a claim while 60% have at least one claim.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" title="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb244-2" title="2">index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> auto_claim<span class="op">$</span>CLM_FLAG, </a>
<a class="sourceLine" id="cb244-3" title="3">                             <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> F) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()</a>
<a class="sourceLine" id="cb244-4" title="4">auto_claim &lt;-<span class="st"> </span>auto_claim <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb244-5" title="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">target =</span> <span class="kw">as.factor</span>(<span class="kw">ifelse</span>(CLM_FLAG <span class="op">==</span><span class="st"> &quot;Yes&quot;</span>, <span class="dv">1</span>,<span class="dv">0</span>)))</a>
<a class="sourceLine" id="cb244-6" title="6">train &lt;-<span class="st">  </span>auto_claim <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(index)</a>
<a class="sourceLine" id="cb244-7" title="7">test &lt;-<span class="st"> </span>auto_claim <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>index)</a>
<a class="sourceLine" id="cb244-8" title="8"></a>
<a class="sourceLine" id="cb244-9" title="9">frequency &lt;-<span class="st"> </span><span class="kw">glm</span>(target <span class="op">~</span><span class="st"> </span>AGE <span class="op">+</span><span class="st"> </span>GENDER <span class="op">+</span><span class="st"> </span>MARRIED <span class="op">+</span><span class="st"> </span>CAR_USE <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb244-10" title="10"><span class="st">                   </span>BLUEBOOK <span class="op">+</span><span class="st"> </span>CAR_TYPE <span class="op">+</span><span class="st"> </span>AREA, </a>
<a class="sourceLine" id="cb244-11" title="11">                 <span class="dt">data=</span>train, </a>
<a class="sourceLine" id="cb244-12" title="12">                 <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>))</a></code></pre></div>
<p>All of the variables except for the <code>CAR_TYPE</code> and <code>GENDERM</code> are highly significant. The car types <code>SPORTS CAR</code> and <code>SUV</code> appear to be significant, and so if we wanted to make the model simpler we could create indicator variables for <code>CAR_TYPE == SPORTS CAR</code> and <code>CAR_TYPE == SUV</code>.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb245-1" title="1">frequency <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = target ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + 
##     CAR_TYPE + AREA, family = binomial(link = &quot;logit&quot;), data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8431  -0.8077  -0.5331   0.9575   3.0441  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        -3.523e-01  2.517e-01  -1.400  0.16160    
## AGE                -2.289e-02  3.223e-03  -7.102 1.23e-12 ***
## GENDERM            -1.124e-02  9.304e-02  -0.121  0.90383    
## MARRIEDYes         -6.028e-01  5.445e-02 -11.071  &lt; 2e-16 ***
## CAR_USEPrivate     -1.008e+00  6.569e-02 -15.350  &lt; 2e-16 ***
## BLUEBOOK           -4.025e-05  4.699e-06  -8.564  &lt; 2e-16 ***
## CAR_TYPEPickup     -6.687e-02  1.390e-01  -0.481  0.63048    
## CAR_TYPESedan      -3.689e-01  1.383e-01  -2.667  0.00765 ** 
## CAR_TYPESports Car  6.159e-01  1.891e-01   3.256  0.00113 ** 
## CAR_TYPESUV         2.982e-01  1.772e-01   1.683  0.09240 .  
## CAR_TYPEVan        -8.983e-03  1.319e-01  -0.068  0.94569    
## AREAUrban           2.128e+00  1.064e-01  19.993  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9544.3  on 8236  degrees of freedom
## Residual deviance: 8309.6  on 8225  degrees of freedom
## AIC: 8333.6
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The signs of the coefficients tell if the probability of having a claim is either increasing or decreasing by each variable. For example, the likelihood of an accident</p>
<ul>
<li>Decreases as the age of the car increases</li>
<li>Is lower for men</li>
<li>Is higher for sports cars and SUVs</li>
</ul>
<p>The p-values tell us if the variable is significant.</p>
<ul>
<li><code>Age</code>, <code>MarriedYes</code>, <code>CAR_USEPrivate</code>, <code>BLUEBOOK</code>, and <code>AreaUrban</code> are significant.</li>
<li>Certain values of <code>CAR_TYPE</code> are significant but others are not.</li>
</ul>
<p>The output is a predicted probability. We can see that this is centered around a probability of about 0.5.</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb247-1" title="1">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(frequency, <span class="dt">newdat=</span>test,<span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</a>
<a class="sourceLine" id="cb247-2" title="2"><span class="kw">qplot</span>(preds) </a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-56"></span>
<img src="05-linear-models_files/figure-html/unnamed-chunk-56-1.png" alt="Distribution of Predicted Probability" width="480" />
<p class="caption">
Figure 13.2: Distribution of Predicted Probability
</p>
</div>
<p>In order to convert these values to predicted 0’s and 1’s, we assign a <em>cutoff</em> value so that if <span class="math inline">\(\hat{y}\)</span> is above this threshold we use a 1 and 0 othersise. The default cutoff is 0.5. We change this to 0.3 and see that there are 763 policies predicted to have claims.</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb248-1" title="1">test &lt;-<span class="st"> </span>test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">pred_zero_one =</span> <span class="kw">as.factor</span>(<span class="dv">1</span><span class="op">*</span>(preds<span class="op">&gt;</span>.<span class="dv">3</span>)))</a>
<a class="sourceLine" id="cb248-2" title="2"><span class="kw">summary</span>(test<span class="op">$</span>pred_zero_one)</a></code></pre></div>
<pre><code>##    0    1 
## 1296  763</code></pre>
<p>How do we decide on this cutoff value? We need to compare cutoff values based on some evaluation metric. For example, we can use <em>accuracy</em>.</p>
<p><span class="math display">\[\text{Accuracy} = \frac{\text{Correct Guesses}}{\text{Total Guesses}}\]</span></p>
<p>This results in an accuracy of 70%. But is this good?</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" title="1">test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(pred_zero_one <span class="op">==</span><span class="st"> </span>target))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   accuracy
##      &lt;dbl&gt;
## 1    0.699</code></pre>
<p>Consider what would happen if we just predicted all 0’s. The accuracy is 74%.</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" title="1">test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(<span class="dv">0</span> <span class="op">==</span><span class="st"> </span>target))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   accuracy
##      &lt;dbl&gt;
## 1    0.734</code></pre>
<p>For policies which experience claims the accuracy is 63%.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb254-1" title="1">test <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-2" title="2"><span class="st">  </span><span class="kw">filter</span>(target <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-3" title="3"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(pred_zero_one <span class="op">==</span><span class="st"> </span>target))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   accuracy
##      &lt;dbl&gt;
## 1    0.631</code></pre>
<p>But for policies that don’t actually experience claims this is 72%.</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb256-1" title="1">test <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb256-2" title="2"><span class="st">  </span><span class="kw">filter</span>(target <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb256-3" title="3"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(pred_zero_one <span class="op">==</span><span class="st"> </span>target))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   accuracy
##      &lt;dbl&gt;
## 1    0.724</code></pre>
<p>How do we know if this is a good model? We can repeat this process with a different cutoff value and get different accuracy metrics for these groups. Let’s use a cutoff of 0.6.</p>
<p>75%</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb258-1" title="1">test &lt;-<span class="st"> </span>test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">pred_zero_one =</span> <span class="kw">as.factor</span>(<span class="dv">1</span><span class="op">*</span>(preds<span class="op">&gt;</span>.<span class="dv">6</span>)))</a>
<a class="sourceLine" id="cb258-2" title="2">test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(pred_zero_one <span class="op">==</span><span class="st"> </span>target))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   accuracy
##      &lt;dbl&gt;
## 1    0.752</code></pre>
<p>10% for policies with claims and 98% for policies without claims.</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" title="1">test <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb260-2" title="2"><span class="st">  </span><span class="kw">filter</span>(target <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb260-3" title="3"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(pred_zero_one <span class="op">==</span><span class="st"> </span>target))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   accuracy
##      &lt;dbl&gt;
## 1    0.108</code></pre>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" title="1">test <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb262-2" title="2"><span class="st">  </span><span class="kw">filter</span>(target <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb262-3" title="3"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(pred_zero_one <span class="op">==</span><span class="st"> </span>target))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   accuracy
##      &lt;dbl&gt;
## 1    0.985</code></pre>
<p>The punchline is that the accuracy depends on the cutoff value, and changing the cutoff value changes whether the model is accuracy for the “true = 1” classes (policies with actual claims) vs. the “false = 0” classes (policies without claims).</p>
</div>
<div id="classification-metrics" class="section level2">
<h2><span class="header-section-number">13.3</span> Classification metrics</h2>
<p>For regression problems, when the output is a whole number, we can use the sum of squares <span class="math inline">\(\text{RSS}\)</span>, the r-squared <span class="math inline">\(R^2\)</span>, the mean absolute error <span class="math inline">\(\text{MAE}\)</span>, and the likelihood. For classification problems where the output is in <span class="math inline">\(\{0,1\}\)</span>, we need to a new set of metrics.</p>
<p>A <em>confusion matrix</em> shows is a table that summarises how the model classifies each group.</p>
<ul>
<li>No claims and predicted to not have claims - <strong>True Negatives (TN) = 1,489</strong></li>
<li>Had claims and predicted to have claims - <strong>True Positives (TP) = 59</strong></li>
<li>No claims but predited to have claims - <strong>False Positives (FP) = 22</strong></li>
<li>Had claims but predicted not to - <strong>False Negatives (FN) = 489</strong></li>
</ul>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" title="1"><span class="kw">confusionMatrix</span>(test<span class="op">$</span>pred_zero_one,<span class="kw">factor</span>(test<span class="op">$</span>target))<span class="op">$</span>table</a></code></pre></div>
<pre><code>##           Reference
## Prediction    0    1
##          0 1489  489
##          1   22   59</code></pre>
<p>These definitions allow us to measure performance on the different groups.</p>
<p><em>Precision</em> answers the question “out of all of the positive predictions, what percentage were correct?”</p>
<p><span class="math display">\[\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\]</span></p>
<p><em>Recall</em> answers the question “out of all of positive examples in the data set, what percentage were correct?”</p>
<p><span class="math display">\[\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\]</span></p>
<p>The choice of using precision vs. recall depends on the relative cost of making a FP or a FN error. If FP errors are expensive, then use precision; if FN errors are expensive, then use recall.</p>
<p><strong>Example A:</strong> the model trying to detect a deadly disease, which only 1 out of every 1000 patient’s survive without early detection. Then the goal should be to optimize <em>recall</em>, because we would want every patient that has the disease to get detected.</p>
<p><strong>Example B:</strong> the model is detecting which emails are spam or not. If an important email is flagged as spam incorrectly, the cost is 5 hours of lost productivity. In this case, <em>precision</em> is the main concern.</p>
<p>In some cases we can compare this “cost” in actual values. For example, if a federal court is predicting if a criminal will recommit or not, they can agree that “1 out of every 20 guilty individuals going free” in exchange for “90% of those who are guilty being convicted”. When money is involed, this a dollar amount can be used: flagging non-spam as spam may cost $100 whereas missing a spam email may cost $2. Then the cost-weighted accuracy is</p>
<p><span class="math display">\[\text{Cost} = (100)(\text{FN}) + (2)(\text{FP})\]</span></p>
<p>Then the cutoff value can be tuned in order to find the minimum cost.</p>
<p>Fortunately, all of this is handled in a single function called <code>confusionMatrix</code>.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" title="1"><span class="kw">confusionMatrix</span>(test<span class="op">$</span>pred_zero_one,<span class="kw">factor</span>(test<span class="op">$</span>target))</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1489  489
##          1   22   59
##                                           
##                Accuracy : 0.7518          
##                  95% CI : (0.7326, 0.7704)
##     No Information Rate : 0.7339          
##     P-Value [Acc &gt; NIR] : 0.03366         
##                                           
##                   Kappa : 0.1278          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt; 2e-16         
##                                           
##             Sensitivity : 0.9854          
##             Specificity : 0.1077          
##          Pos Pred Value : 0.7528          
##          Neg Pred Value : 0.7284          
##              Prevalence : 0.7339          
##          Detection Rate : 0.7232          
##    Detection Prevalence : 0.9607          
##       Balanced Accuracy : 0.5466          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<div id="area-under-the-roc-curv-auc" class="section level3">
<h3><span class="header-section-number">13.3.1</span> Area Under the ROC Curv (AUC)</h3>
<p>What if we look at both the true-positive rate (TPR) and false positive rate (FPR) simultaneously? That is, for each value of the cutoff, we can calculate the TPR and TNR.</p>
<p>For example, say that we have 10 cutoff values, <span class="math inline">\(\{k_1, k_2, ..., k_{10}\}\)</span>. Then for each value of <span class="math inline">\(k\)</span> we calculate both the true positive rates</p>
<p><span class="math display">\[\text{TPR} = \{\text{TPR}(k_1), \text{TPR}(k_2), .., \text{TPR}(k_{10})\} \]</span></p>
<p>and the true negative rates</p>
<p><span class="math display">\[\{\text{FNR} = \{\text{FNR}(k_1), \text{FNR}(k_2), .., \text{FNR}(k_{10})\}\]</span></p>
<p>Then we set <code>x = TPR</code> and <code>y = FNR</code> and graph x against y. The plot below shows the ROC for the <code>auto_claims</code> data. The Area Under the Curv of 0.6795 is what we would get if we integrated under the curve.</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb268-1" title="1"><span class="kw">library</span>(pROC)</a>
<a class="sourceLine" id="cb268-2" title="2"><span class="kw">roc</span>(test<span class="op">$</span>target, preds, <span class="dt">plot =</span> T)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-66"></span>
<img src="05-linear-models_files/figure-html/unnamed-chunk-66-1.png" alt="AUC for auto_claim" width="672" />
<p class="caption">
Figure 13.3: AUC for auto_claim
</p>
</div>
<pre><code>## 
## Call:
## roc.default(response = test$target, predictor = preds, plot = T)
## 
## Data: preds in 1511 controls (test$target 0) &lt; 548 cases (test$target 1).
## Area under the curve: 0.7558</code></pre>
<p>If we just randomly guess, the AUC would be 0.5, which is represented by the 45-degree line. A perfect model would maximize the curve to the upper-left corner.</p>
<p>AUC is preferred over Accuracy when there are a lot more “true” classes than “false” classes, which is known as having **class imbalance*. An example is bank fraud detection: 99.99% of bank transactions are “false” or “0” classes, and so optimizing for accuracy alone will result in a low sensitivity for detecting actual fraud.</p>
</div>
<div id="additional-reading" class="section level3">
<h3><span class="header-section-number">13.3.2</span> Additional reading</h3>
<table>
<thead>
<tr class="header">
<th>Title</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>An Overview of Classification</td>
<td>ISL 4.1</td>
</tr>
<tr class="even">
<td><a href="https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5#:~:targetText=What%20is%20AUC%20%2D%20ROC%20Curve%3F,capable%20of%20distinguishing%20between%20classes.">Understanding AUC - ROC Curv</a></td>
<td>Sarang Narkhede, Towards Data Science</td>
</tr>
<tr class="odd">
<td><a href="https://towardsdatascience.com/precision-vs-recall-386cf9f89488#:~:targetText=Precision%20and%20recall%20are%20two,correctly%20classified%20by%20your%20algorithm.">Precision vs. Recall</a></td>
<td>Shruti Saxena, Towards Data Science</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="glms-for-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="penalized-linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sdcastillo/PA-R-Study-Manual/edit/master/05-linear-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Exam-PA-Study-Manual.pdf", "Exam-PA-Study-Manual.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
