<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Tree-based models | The Predictive Analytics R Study Manual</title>
  <meta name="description" content="This will help you pass these exams" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Tree-based models | The Predictive Analytics R Study Manual" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This will help you pass these exams" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Tree-based models | The Predictive Analytics R Study Manual" />
  
  <meta name="twitter:description" content="This will help you pass these exams" />
  

<meta name="author" content="Sam Castillo" />


<meta name="date" content="2019-10-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="visualization.html"/>
<link rel="next" href="a-mini-exam-example.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a></li>
<li class="chapter" data-level="2" data-path="the-exam.html"><a href="the-exam.html"><i class="fa fa-check"></i><b>2</b> The Exam</a></li>
<li class="chapter" data-level="3" data-path="preface-what-is-machine-learning.html"><a href="preface-what-is-machine-learning.html"><i class="fa fa-check"></i><b>3</b> Preface - What is Machine Learning?</a></li>
<li class="chapter" data-level="4" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>4</b> Getting started</a><ul>
<li class="chapter" data-level="4.1" data-path="getting-started.html"><a href="getting-started.html#download-islr"><i class="fa fa-check"></i><b>4.1</b> Download ISLR</a></li>
<li class="chapter" data-level="4.2" data-path="getting-started.html"><a href="getting-started.html#installing-r"><i class="fa fa-check"></i><b>4.2</b> Installing R</a></li>
<li class="chapter" data-level="4.3" data-path="getting-started.html"><a href="getting-started.html#installing-rstudio"><i class="fa fa-check"></i><b>4.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="4.4" data-path="getting-started.html"><a href="getting-started.html#set-the-r-library"><i class="fa fa-check"></i><b>4.4</b> Set the R library</a></li>
<li class="chapter" data-level="4.5" data-path="getting-started.html"><a href="getting-started.html#download-the-data"><i class="fa fa-check"></i><b>4.5</b> Download the data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="r-programming.html"><a href="r-programming.html"><i class="fa fa-check"></i><b>5</b> R programming</a><ul>
<li class="chapter" data-level="5.1" data-path="r-programming.html"><a href="r-programming.html#notebook-chunks"><i class="fa fa-check"></i><b>5.1</b> Notebook chunks</a></li>
<li class="chapter" data-level="5.2" data-path="r-programming.html"><a href="r-programming.html#basic-operations"><i class="fa fa-check"></i><b>5.2</b> Basic operations</a></li>
<li class="chapter" data-level="5.3" data-path="r-programming.html"><a href="r-programming.html#lists"><i class="fa fa-check"></i><b>5.3</b> Lists</a></li>
<li class="chapter" data-level="5.4" data-path="r-programming.html"><a href="r-programming.html#functions"><i class="fa fa-check"></i><b>5.4</b> Functions</a></li>
<li class="chapter" data-level="5.5" data-path="r-programming.html"><a href="r-programming.html#data-frames"><i class="fa fa-check"></i><b>5.5</b> Data frames</a></li>
<li class="chapter" data-level="5.6" data-path="r-programming.html"><a href="r-programming.html#pipes"><i class="fa fa-check"></i><b>5.6</b> Pipes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>6</b> Data manipulation</a><ul>
<li class="chapter" data-level="6.1" data-path="data-manipulation.html"><a href="data-manipulation.html#look-at-the-data"><i class="fa fa-check"></i><b>6.1</b> Look at the data</a></li>
<li class="chapter" data-level="6.2" data-path="data-manipulation.html"><a href="data-manipulation.html#transform-the-data"><i class="fa fa-check"></i><b>6.2</b> Transform the data</a></li>
<li class="chapter" data-level="6.3" data-path="data-manipulation.html"><a href="data-manipulation.html#exercises"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
<li class="chapter" data-level="6.4" data-path="data-manipulation.html"><a href="data-manipulation.html#answers-to-exercises"><i class="fa fa-check"></i><b>6.4</b> Answers to exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>7</b> Visualization</a><ul>
<li class="chapter" data-level="7.1" data-path="visualization.html"><a href="visualization.html#step-1.-put-the-data-in-a-pivotable-format"><i class="fa fa-check"></i><b>7.1</b> Step 1. Put the data in a pivotable format</a><ul>
<li class="chapter" data-level="7.1.1" data-path="visualization.html"><a href="visualization.html#example-of-wide-or-matrix-format"><i class="fa fa-check"></i><b>7.1.1</b> Example of “wide” or “matrix” format</a></li>
<li class="chapter" data-level="7.1.2" data-path="visualization.html"><a href="visualization.html#example-of-pivotable-long-or-tidy-format"><i class="fa fa-check"></i><b>7.1.2</b> Example of “pivotable”, “long”, or “tidy” format</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="visualization.html"><a href="visualization.html#step-2.-create-a-plot-object-ggplot"><i class="fa fa-check"></i><b>7.2</b> Step 2. Create a plot object (ggplot)</a></li>
<li class="chapter" data-level="7.3" data-path="visualization.html"><a href="visualization.html#step-3-add-a-plot"><i class="fa fa-check"></i><b>7.3</b> Step 3: Add a plot</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>8</b> Tree-based models</a><ul>
<li class="chapter" data-level="8.1" data-path="tree-based-models.html"><a href="tree-based-models.html#decision-trees"><i class="fa fa-check"></i><b>8.1</b> Decision Trees</a></li>
<li class="chapter" data-level="8.2" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>8.2</b> Advantages and disadvantages</a></li>
<li class="chapter" data-level="8.3" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>8.3</b> Random Forests</a></li>
<li class="chapter" data-level="8.4" data-path="tree-based-models.html"><a href="tree-based-models.html#gradient-boosted-trees"><i class="fa fa-check"></i><b>8.4</b> Gradient Boosted Trees</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html"><i class="fa fa-check"></i><b>9</b> A Mini-Exam Example</a><ul>
<li class="chapter" data-level="9.1" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#project-statement"><i class="fa fa-check"></i><b>9.1</b> Project Statement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#describe-the-data-1-point"><i class="fa fa-check"></i><b>9.1.1</b> Describe the data (1 point)</a></li>
<li class="chapter" data-level="9.1.2" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#create-a-histogram-of-the-claims-and-comment-on-the-shape-1-point"><i class="fa fa-check"></i><b>9.1.2</b> Create a histogram of the claims and comment on the shape (1 point)</a></li>
<li class="chapter" data-level="9.1.3" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#fit-a-linear-model-1-point"><i class="fa fa-check"></i><b>9.1.3</b> Fit a linear model (1 point)</a></li>
<li class="chapter" data-level="9.1.4" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#describe-the-relationship-between-age-sex-and-claim-costs-1-point"><i class="fa fa-check"></i><b>9.1.4</b> Describe the relationship between age, sex, and claim costs (1 point)</a></li>
<li class="chapter" data-level="9.1.5" data-path="a-mini-exam-example.html"><a href="a-mini-exam-example.html#write-a-summary-of-steps-1-4-in-non-technical-language-1-point"><i class="fa fa-check"></i><b>9.1.5</b> Write a summary of steps 1-4 in non-technical language (1 point)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="practice-exam.html"><a href="practice-exam.html"><i class="fa fa-check"></i><b>10</b> Practice Exam</a></li>
<li class="chapter" data-level="11" data-path="prior-exams.html"><a href="prior-exams.html"><i class="fa fa-check"></i><b>11</b> Prior Exams</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Predictive Analytics R Study Manual</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-models" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Tree-based models</h1>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">8.1</span> Decision Trees</h2>
<p>Decision trees can be used for either classification or regression problems. The model structure is a series of yes/no questions. Depending on how each observation answers these questions, a prediction is made.</p>
<p>The below example shows how a single tree can predict health claims.</p>
<ul>
<li>For non-smokers, the predicted annual claims are 8,434. This represents 80% of the observations</li>
<li>For smokers with a <code>bmi</code> of less than 30, the predicted annual claims are 21,000. 10% of patients fall into this bucket.</li>
<li>For smokers with a <code>bmi</code> of more than 30, the prediction is 42,000. This bucket accounts for 11% of patients.</li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="06-tree-based-models_files/figure-html/unnamed-chunk-2-1.png" alt="Decision tree of health costs" width="672" />
<p class="caption">
Figure 8.1: Decision tree of health costs
</p>
</div>
<p>We can cut the data set up into these groups and look at the claim costs. From this grouping, we can see that <code>smoker</code> is the most important variable as the difference in average claims is about 20,000.</p>
<table>
<thead>
<tr class="header">
<th align="left">smoker</th>
<th align="left">bmi_30</th>
<th align="left">mean_claims</th>
<th align="right">percent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">no</td>
<td align="left">bmi &lt; 30</td>
<td align="left">$7,977.03</td>
<td align="right">0.38</td>
</tr>
<tr class="even">
<td align="left">no</td>
<td align="left">bmi &gt;= 30</td>
<td align="left">$8,842.69</td>
<td align="right">0.42</td>
</tr>
<tr class="odd">
<td align="left">yes</td>
<td align="left">bmi &lt; 30</td>
<td align="left">$21,363.22</td>
<td align="right">0.10</td>
</tr>
<tr class="even">
<td align="left">yes</td>
<td align="left">bmi &gt;= 30</td>
<td align="left">$41,557.99</td>
<td align="right">0.11</td>
</tr>
</tbody>
</table>
<p>This was a very simple example because there were only two variables. If we have more variables, the tree will get large very quickly. This will result in overfitting; there will be good performance on the training data but poor performance on the test data.</p>
<p>The step-by-step process of building a tree is</p>
<p><strong>Step 1: Choose a variable at random.</strong></p>
<p>This could be any variable in <code>age</code>, <code>children</code>, <code>charges</code>, <code>sex</code>, <code>smoker</code>, <code>age_bucket</code>, <code>bmi</code>, or <code>region</code>.</p>
<p><strong>Step 2: Find the split point which best seperates observations out based on the value of <span class="math inline">\(y\)</span>. A good split is one where the <span class="math inline">\(y\)</span>’s are very different. * </strong></p>
<p>In this case, <code>smoker</code> was chosen. Then we can only split this in one way: <code>smoker = 1</code> or <code>smoker = 0</code>.</p>
<p>Then for each of these groups, smokers and non-smokers, choose another variable at random. In this case, for no-smokers, <code>age</code> was chosen. To find the best cut point of <code>age</code>, look at all possible age cut points from 18, 19, 20, 21, …, 64 and choose the one which best separates the data.</p>
<p>There are three ways of deciding where to split</p>
<ul>
<li><em>Entropy</em> (aka, information gain)</li>
<li><em>Gini</em></li>
<li><em>Classification error</em></li>
</ul>
<p>Of these, only the first two are commonly used. The exam is not going to ask you to calculate either of these. Just know that neither method will work better on all data sets, and so the best practice is to test both and compare the performance.</p>
<p><strong>Step 3: Continue doing this until a stopping criteria is reached. For example, the minimum number of observations is 5 or less.</strong></p>
<p>As you can see, this results in a very deep tree.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" title="1">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> charges <span class="op">~</span><span class="st">  </span>., <span class="dt">data =</span> health_insurance,</a>
<a class="sourceLine" id="cb173-2" title="2">              <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="fl">0.003</span>))</a>
<a class="sourceLine" id="cb173-3" title="3"><span class="kw">rpart.plot</span>(tree, <span class="dt">type =</span> <span class="dv">3</span>)</a></code></pre></div>
<p><img src="06-tree-based-models_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p><strong>Step 4: Apply cost comlexity pruning to simplify the tree</strong></p>
<p>Intuitively, we know that the above model would perform poorly due to overfitting. We want to make it simpler by removing nodes. This is very similar to how in linear models we reduce complexity by reducing the number of coefficients.</p>
<p>A measure of the depth of the tree is the <em>complexity</em>. You can think of <span class="math inline">\(|T|\)</span> as the “degrees of freedom” in a linear model. In the above example, <span class="math inline">\(|T| = 8\)</span>. The amount of penalization is controlled by <span class="math inline">\(\alpha\)</span>. This is very similar to <span class="math inline">\(\lambda\)</span> in the Lasso.</p>
<p>To calculate the cost of a tree, number the terminal nodes from <span class="math inline">\(1\)</span> to <span class="math inline">\(|T|\)</span>, and let the set of observations that fall into the <span class="math inline">\(mth\)</span> bucket be <span class="math inline">\(R_m\)</span>. Then add up the squared error over all terminal nodes to the penalty term.</p>
<p><span class="math display">\[
\text{Cost}_\alpha(T) = \sum_{m=1}^{|T|} \sum_{R_m}(y_i - \hat{y}_{Rm})^2 + \alpha |T|
\]</span></p>
<p><strong>Step 5: Use cross-validation to select the best alpha</strong></p>
<p>The cost is controlled by the <code>CP</code> parameter. In the above example, did you notice the line <code>rpart.control(cp = 0.003)</code>? This is telling <code>rpart</code> to continue growing the tree until the CP reaches 0.003. At each subtree, we can measure the cost <code>CP</code> as well as the cross-validation error <code>xerror</code>.</p>
<p>This is stored in the <code>cptable</code></p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" title="1"><span class="kw">library</span>(kableExtra)</a>
<a class="sourceLine" id="cb174-2" title="2"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb174-3" title="3">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> charges <span class="op">~</span><span class="st">  </span>., <span class="dt">data =</span> health_insurance,</a>
<a class="sourceLine" id="cb174-4" title="4">              <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="fl">0.0001</span>))</a>
<a class="sourceLine" id="cb174-5" title="5">cost &lt;-<span class="st"> </span>tree<span class="op">$</span>cptable <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb174-6" title="6"><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb174-7" title="7"><span class="st">  </span><span class="kw">select</span>(nsplit, CP, xerror) </a>
<a class="sourceLine" id="cb174-8" title="8"></a>
<a class="sourceLine" id="cb174-9" title="9">cost <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()</a></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##   nsplit      CP xerror
##    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1      0 0.620    1.00 
## 2      1 0.144    0.381
## 3      2 0.0637   0.240
## 4      3 0.00967  0.180
## 5      4 0.00784  0.174
## 6      5 0.00712  0.171</code></pre>
<p>As more splits are added, the cost continues to decrease, reaches a minimum, and then begins to increase.</p>
<p><img src="06-tree-based-models_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>To optimize performance, choose the number of splits which has the lowest error. Often, though, the goal of using a decision tree is to create a simple model. In this case, we can err or the side of a lower <code>nsplit</code> so that the tree is shorter and more interpretable. All of the questions on so far have only used decision trees for interpretability, and a different model method has been used when predictive power is needed.</p>
<p>You will typically be given the below code, which does this automatically. To get full credit on decision tree questions, mention that you used cross-validation to select the number of splits.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" title="1">pruned_tree &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp =</span> tree<span class="op">$</span>cptable[<span class="kw">which.min</span>(tree<span class="op">$</span>cptable[, <span class="st">&quot;xerror&quot;</span>]), <span class="st">&quot;CP&quot;</span>])</a></code></pre></div>
</div>
<div id="advantages-and-disadvantages" class="section level2">
<h2><span class="header-section-number">8.2</span> Advantages and disadvantages</h2>
<p><strong>Advantages</strong></p>
<ul>
<li>Easy to interpret</li>
<li>Captures interaction effects</li>
<li>Captures non-linearities</li>
<li>Handles continuous and categorical data</li>
<li>Handles missing values</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>Is a “weak learner” because of low predictive power</li>
<li>Does not work on small data sets</li>
<li>Is often a simplification of the underlying process because all observations at terminal nodes have equal predicted values</li>
<li>Is biased towards selecting high-cardinality features because more possible split points for these features tend to lead to overfitting</li>
<li>High variance (which can be alleviated with stricter parameters) leads the “easy to interpret results” to change upon retraining
Unable to predict beyond the range of the training data for regression (because each predicted value is an average of training samples)</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.1.1 Basics of Decision Trees</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 8.1.2 Classification Trees</td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">rpart Documentation (Optional)</a></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">8.3</span> Random Forests</h2>
<p><strong>Advantages</strong></p>
<ul>
<li>Resilient to overfitting due to bagging</li>
<li>Only one parameter to tune (mtry, the number of features considered at each split)</li>
<li>Very good a multi-class prediction</li>
<li>Nonlinearities</li>
<li>Interaction effects</li>
<li>Deal with unbalanced and missing data*Usually requires over/undersamplin</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>Does not work on small data sets</li>
<li>Weaker performance than other methods (GBM, NN)</li>
<li>Unable to predict beyond training data for regression</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.1.1 Basics of Decision Trees</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 8.1.2 Classification Trees</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="gradient-boosted-trees" class="section level2">
<h2><span class="header-section-number">8.4</span> Gradient Boosted Trees</h2>
<ul>
<li>High prediction accuracy</li>
<li>Closest model to a “silver bullet” that exists</li>
<li>Nonlinearities, interaction effects, resilient to outliers, corrects for missing values</li>
<li>Deals with class imbalance directly through by weighting observations</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li>Requires large sample size</li>
<li>Longer training time</li>
<li>Does not detect linear combinations of features. These must be engineered
Can overfit if not tuned correctly</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Readings</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ISLR 8.1.1 Basics of Decision Trees</td>
<td></td>
</tr>
<tr class="even">
<td>ISLR 8.1.2 Classification Trees</td>
<td></td>
</tr>
</tbody>
</table>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="visualization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-mini-exam-example.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-tree-based-models.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Exam PA Study Manual.pdf", "Exam PA Study Manual.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
